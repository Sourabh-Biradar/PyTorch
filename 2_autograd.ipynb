{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X36Y5G-1rmzC"
      },
      "outputs": [],
      "source": [
        "# PyTorch - 2\n",
        "\n",
        "# Autograd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "cfUXGzORrrqV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example 1 : calculating gradient\n",
        "\n",
        "x = torch.tensor(3.0,requires_grad=True)\n",
        "\n",
        "# requires_grad = True : mandatory"
      ],
      "metadata": {
        "id": "h7zrgfdjrskF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x**2\n",
        "\n",
        "print(y)\n",
        "\n",
        "y.backward()\n",
        "\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjy4Mr-hsEQN",
        "outputId": "22b85cdb-36a5-45e0-e07f-3480df084552"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9., grad_fn=<PowBackward0>)\n",
            "tensor(6.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ex : 2\n",
        "\n",
        "a = torch.tensor(4.0,requires_grad=True)\n",
        "\n",
        "b = a**2\n",
        "\n",
        "c = torch.sin(b)\n",
        "\n",
        "c.backward()              # dc/da = dc/db . db/da\n",
        "\n",
        "print(a.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B47d4v7gsNlw",
        "outputId": "1223f569-5a71-497a-90e9-9d186be8c29d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-7.6613)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ex : 3 with vector input\n",
        "\n",
        "x = torch.tensor([1.0,2.0,3.0],requires_grad=True)    # x = x1,x2,x3\n",
        "\n",
        "y = (x**2).mean()       # applying mean() fn : y = fn(x1,x2,x3)\n",
        "\n",
        "y.backward()            # Dy = [dy/dx1 ,dy/dx2, dy/dx3]\n",
        "\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdECBs98vo7j",
        "outputId": "fdf3e57f-b223-4265-c1e0-3d11280dc26a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6667, 1.3333, 2.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clearing grads\n",
        "\n",
        "# if we dont clear gradients they start accumalating everytime we call backward()\n",
        "# NEED TO DO it AFTER EVERY EPOCH\n",
        "\n",
        "x.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob0PYnde5OsR",
        "outputId": "50f2bd6b-7a65-4508-f42e-849350600207"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# disabling gradient tracking\n",
        "\n",
        "# once NN is trained , we dont need backward() to predict\n",
        "# need to disable it as it takes memory\n",
        "\n",
        "x.requires_grad_(False)\n",
        "\n",
        "# y.backward() gives ERROR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV4MWPSU5SBZ",
        "outputId": "4f512e11-3264-4043-9e49-7e39227c25d4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# way 2\n",
        "\n",
        "with torch.no_grad():\n",
        "    y = x**2\n",
        "   #  y.backward()        # ERROR\n",
        "\n"
      ],
      "metadata": {
        "id": "C1pI6deS6dsQ"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}